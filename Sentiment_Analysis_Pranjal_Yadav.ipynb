{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c740f1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Product Reviews using Naive Bayes\n",
    "\n",
    "**Author:** Pranjal Yadav\n",
    "\n",
    "This notebook demonstrates a full pipeline for sentiment analysis on product reviews using Python: data loading, preprocessing, TF-IDF vectorization, training a Naive Bayes classifier, evaluation, and saving the model. Replace `reviews.csv` with your dataset (CSV with columns: `review`, `sentiment`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35a20e",
   "metadata": {},
   "source": [
    "## 1. Install required packages\n",
    "\n",
    "If you are running locally, install required packages with:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "(Requirements file is included.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data if needed\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18657e",
   "metadata": {},
   "source": [
    "## 3. Load dataset\n",
    "\n",
    "Place a CSV file named `reviews.csv` in the same folder with two columns: `review` and `sentiment`.\n",
    "`sentiment` should be categorical: `positive`, `negative`, or `neutral` (or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24725572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: load reviews.csv\n",
    "# If you don't have a dataset, you can create a small example DataFrame (uncomment the example).\n",
    "import os\n",
    "\n",
    "if not os.path.exists('reviews.csv'):\n",
    "    # Create a small example dataset\n",
    "    data = {\n",
    "        'review': [\n",
    "            'Great phone with long battery life',\n",
    "            'Very poor build quality, stopped working in a week',\n",
    "            'Average product, does the job',\n",
    "            'Excellent! Highly recommended',\n",
    "            'Not worth the money'\n",
    "        ],\n",
    "        'sentiment': ['positive', 'negative', 'neutral', 'positive', 'negative']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('reviews.csv', index=False)\n",
    "    print('Example reviews.csv file created.')\n",
    "else:\n",
    "    df = pd.read_csv('reviews.csv')\n",
    "    print('reviews.csv loaded, shape:', pd.read_csv('reviews.csv').shape)\n",
    "\n",
    "df = pd.read_csv('reviews.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e24734",
   "metadata": {},
   "source": [
    "## 4. Text preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    # remove URLs, HTML tags, non-alphabetic\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['clean_review'] = df['review'].astype(str).apply(preprocess_text)\n",
    "df[['review', 'clean_review', 'sentiment']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d640e",
   "metadata": {},
   "source": [
    "## 5. Train/Test split and label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c79257",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_review']\n",
    "y = df['sentiment']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.2, random_state=42, stratify=y_enc)\n",
    "print('Train size:', len(X_train), 'Test size:', len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3ec41",
   "metadata": {},
   "source": [
    "## 6. Build pipeline (TF-IDF + Multinomial Naive Bayes) and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd02c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae8adc",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification Report:\\n', classification_report(y_test, y_pred, target_names=le.inverse_transform(sorted(set(y_test)))))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea91b4",
   "metadata": {},
   "source": [
    "## 8. Save trained model and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395f739",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipeline, 'sentiment_pipeline.pkl')\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "print('Saved: sentiment_pipeline.pkl and label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f545693d",
   "metadata": {},
   "source": [
    "## 9. Make predictions on new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    clean = preprocess_text(text)\n",
    "    pred = pipeline.predict([clean])[0]\n",
    "    return le.inverse_transform([pred])[0]\n",
    "\n",
    "samples = [\n",
    "    'I love this product, fantastic value.',\n",
    "    'Terrible experience, broke after one use.',\n",
    "    'Okay product, nothing special.'\n",
    "]\n",
    "\n",
    "for s in samples:\n",
    "    print(s, '->', predict_sentiment(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26fc46",
   "metadata": {},
   "source": [
    "## Notes & Next steps\n",
    "\n",
    "- Replace `reviews.csv` with your real dataset (columns `review`, `sentiment`).\n",
    "- You can expand preprocessing (lemmatization, more cleaning) or try other classifiers (Logistic Regression, SVM).\n",
    "- To run on GitHub: push this notebook and the `reviews.csv` (or instructions) to a repository. CI won't run notebooks automatically â€” use GitHub Codespaces or Binder if you want runnable cloud notebooks."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
